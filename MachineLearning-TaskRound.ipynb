{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e57ac40",
   "metadata": {},
   "source": [
    "# Task-1:\n",
    "Choose any one deep learning algorithm of your choice and try to\n",
    "implement it from scratch.\n",
    "Important points:\n",
    "<ul>\n",
    "● You can use modules like JAX, Numpy, etc. for your\n",
    "implementations.<br>\n",
    "● You should be able to explain the mathematical concept\n",
    "behind your implementation.<br>\n",
    " </ul>\n",
    "Judging Criteria:<br>\n",
    "● Structure of your code.<br>\n",
    "● Math and code implementation.<br>\n",
    "<br>\n",
    "Take the given Implementation as a reference: <a href=\"https://github.com/Math-behind-AI/ScratchAI/tree/main/traditional_ML_algorithms\">Link </a><br>\n",
    "\n",
    "Note: For your reference, the link attached herewith shows implementations\n",
    "of a few ML algorithms. However, the task is to implement DL algorithms\n",
    "from scratch.<br>\n",
    "\n",
    "A few of the DL algorithms you can implement but are not limited\n",
    "to are Multi-layer perceptron, Convolutional Neural Nets,\n",
    "Recurrent Neural Nets, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ce6b1",
   "metadata": {},
   "source": [
    "## What is Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d28bf",
   "metadata": {},
   "source": [
    "## Working of MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4d52b",
   "metadata": {},
   "source": [
    "The Basic steps are as follows:<br>\n",
    "<ol>\n",
    "    <li>Initialize the weights and bias with small-randomized values</li>\n",
    "    <li>Propagate all values in the input layer until output layer(Forward Propagation)</li>\n",
    "    <li>Update weight and bias in the inner layers(Backpropagation)</li>\n",
    "    <li>Do it until that the stop criterion is satisfied !</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733b493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "993e122c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df81a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "383b9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "     \n",
    "    def __init__(self,hidden_layer, epoch, learning_rate, verbose=False):\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    # Initializing the weights    \n",
    "    def initial_weights(self, X, y):\n",
    "        n_sample, n_features = X.shape\n",
    "        n_output = y.shape[1]\n",
    "        \n",
    "        limit_hidden = 1/math.sqrt(n_features)\n",
    "        self.hiddenWeight = np.random.uniform(-limit_hidden,limit_hidden, (n_features, self.hidden_layer))        \n",
    "        self.BiasHidden = np.zeros((1,self.hidden_layer))\n",
    "        \n",
    "        limit_out = 1/ math.sqrt(self.hidden_layer)\n",
    "        self.outputWeight = np.random.uniform(-limit_out,limit_out, (self.hidden_layer, n_output))\n",
    "        self.BiasOutput = np.zeros((1, n_output))\n",
    "     \n",
    "    #Sigmoid Function\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    #Sigmoid Derivative Function\n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "     \n",
    "    #SoftMax Function (Output Layer)    \n",
    "    def softmax(self, z):\n",
    "        e_x = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    #SoftMax Gradient Function\n",
    "    def softmax_gradient(self, z):\n",
    "        return self.softmax(z) * (1 - self.softmax(z))\n",
    "    \n",
    "    #Cross-Entropy Loss Function\n",
    "    def loss(self, h, y):\n",
    "        h = np.clip(h, 1e-15, 1 - 1e-15)\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "    \n",
    "    #Cross-Entropy Loss Gradient Function\n",
    "    def loss_gradient(self, h, y):\n",
    "        h = np.clip(h, 1e-15, 1 - 1e-15)\n",
    "        return -(h/y) + (1-h)/(1-y)\n",
    "    \n",
    "    #Accuracy Score Function\n",
    "    def accuracy_score(self, y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "        return accuracy\n",
    "    \n",
    "    #Prediction Function\n",
    "    def predict(self, X):\n",
    "        hidden_input = X.dot(self.hiddenWeight) + self.BiasHidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        output_layer_input = hidden_output.dot(self.outputWeight) + self.BiasOutput\n",
    "        y_pred = self.softmax(output_layer_input)\n",
    "        return y_pred\n",
    "    \n",
    "    #Fit Function\n",
    "    def fit(self, X, y):\n",
    "        self.initial_weights(X, y)\n",
    "        n_epoch = 1\n",
    "        \n",
    "        while(n_epoch <= self.epoch):\n",
    "            \n",
    "            # Forward Propogation\n",
    "            #hidden Layer\n",
    "            hidden_input = X.dot(self.hiddenWeight) + self.BiasHidden\n",
    "            hidden_output = self.sigmoid(hidden_input)\n",
    "            #output layer\n",
    "            output_layer_input = hidden_output.dot(self.outputWeight) + self.BiasOutput\n",
    "            y_pred = self.softmax(output_layer_input)\n",
    "            \n",
    "            #Backward Propogation\n",
    "            #Output Layer Gradient\n",
    "            grad_out_input = self.loss_gradient(y, y_pred) * self.softmax_gradient(output_layer_input)\n",
    "            grad_output = hidden_output.T.dot(grad_out_input)\n",
    "            grad_biasoutput = np.sum(grad_out_input,axis=0,keepdims=True)\n",
    "            #Hidden Layer Gradient\n",
    "            grad_input_out = grad_out_input.dot(self.outputWeight.T) * self.sigmoid_derivative(hidden_input)\n",
    "            grad_input = X.T.dot(grad_input_out)\n",
    "            grad_biasinput = np.sum(grad_input_out, axis=0, keepdims=True)\n",
    "            \n",
    "            #Updating Weights\n",
    "            self.outputWeight -= self.learning_rate * grad_output\n",
    "            self.BiasOutput -= self.learning_rate *grad_biasoutput\n",
    "            self.hiddenWeight -= self.learning_rate * grad_input\n",
    "            self.BiasHidden -= self.learning_rate * grad_biasinput\n",
    "                        \n",
    "            \n",
    "            n_epoch += 1\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "87b5ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = load_digits()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    \n",
    "    #Normalize X\n",
    "    X1 = X.copy()\n",
    "    X1 = (X1 - X1.mean())/X1.std()\n",
    "    \n",
    "    #Categorize Y\n",
    "    y1 = np.zeros((y.shape[0], (np.amax(y)+1)))\n",
    "    y1[np.arange(y.shape[0]), y] = 1\n",
    "    \n",
    "    #Train-Test Split\n",
    "    split_i = len(y1) - int(len(y1) // (1 / 0.2))\n",
    "    X_train, X_test = X1[:split_i], X1[split_i:]\n",
    "    y_train, y_test = y1[:split_i], y1[split_i:]\n",
    "    \n",
    "    \n",
    "    clf = MultiLayerPerceptron(hidden_layer = 10, epoch=1000, learning_rate=0.01, verbose=True)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = np.argmax(clf.predict(X_test), axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuracy = clf.accuracy_score(y_test, y_pred)\n",
    "    print (\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3d23a998",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (359,) (64,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [129]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [128]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(clf\u001b[38;5;241m.\u001b[39mpredict(X_test), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss1\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss1)\n\u001b[0;32m     28\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39maccuracy_score(y_test, y_pred)\n",
      "Input \u001b[1;32mIn [127]\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.loss1\u001b[1;34m(self, y_pred)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss1\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_pred):\n\u001b[0;32m     48\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhiddenWeight\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [127]\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.loss\u001b[1;34m(self, h, y)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, h, y):\n\u001b[0;32m     36\u001b[0m     h \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(h, \u001b[38;5;241m1e-15\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1e-15\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m h))\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (359,) (64,10) "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c4d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df4226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
