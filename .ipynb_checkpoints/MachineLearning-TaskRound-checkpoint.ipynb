{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e57ac40",
   "metadata": {},
   "source": [
    "# Task-1:\n",
    "Choose any one deep learning algorithm of your choice and try to\n",
    "implement it from scratch.\n",
    "Important points:\n",
    "<ul>\n",
    "● You can use modules like JAX, Numpy, etc. for your\n",
    "implementations.<br>\n",
    "● You should be able to explain the mathematical concept\n",
    "behind your implementation.<br>\n",
    " </ul>\n",
    "Judging Criteria:<br>\n",
    "● Structure of your code.<br>\n",
    "● Math and code implementation.<br>\n",
    "<br>\n",
    "Take the given Implementation as a reference: <a href=\"https://github.com/Math-behind-AI/ScratchAI/tree/main/traditional_ML_algorithms\">Link </a><br>\n",
    "\n",
    "Note: For your reference, the link attached herewith shows implementations\n",
    "of a few ML algorithms. However, the task is to implement DL algorithms\n",
    "from scratch.<br>\n",
    "\n",
    "A few of the DL algorithms you can implement but are not limited\n",
    "to are Multi-layer perceptron, Convolutional Neural Nets,\n",
    "Recurrent Neural Nets, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ce6b1",
   "metadata": {},
   "source": [
    "## What is Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d28bf",
   "metadata": {},
   "source": [
    "## Working of MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df24903",
   "metadata": {},
   "source": [
    "The Basic steps are as follows:<br>\n",
    "<ol>\n",
    "    <li>Initialize the weights and bias with small-randomized values</li>\n",
    "    <li>Propagate all values in the input layer until output layer(Forward Propagation)</li>\n",
    "    <li>Update weight and bias in the inner layers(Backpropagation)</li>\n",
    "    <li>Do it until that the stop criterion is satisfied !</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733b493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "811ffa4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9592e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c9465de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "     \n",
    "    def __init__(self,hidden_layer, epoch, learning_rate, verbose=False):\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        \n",
    "    def initial_weights(self, X, y):\n",
    "        n_sample, n_features = X.shape\n",
    "        n_output = y.shape[1]\n",
    "        limit = 1/math.sqrt(n_features)\n",
    "        \n",
    "        self.hiddenWeight = np.random.uniform(-limit,limit, (n_features, self.hidden_layer))\n",
    "        self.outputWeight = np.random.uniform(-limit,limit, (self.hidden_layer, n_output))\n",
    "        \n",
    "        self.BiasHidden = np.zeros((1,self.hidden_layer))\n",
    "        self.BiasOutput = np.zeros((1, n_output))\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "         \n",
    "    def softmax(self, z):\n",
    "        e_x = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def softmax_gradient(self, z):\n",
    "        return self.softmax(z) * (1 - self.softmax(z))\n",
    "    \n",
    "    def loss(self, h, y):\n",
    "        h = np.clip(h, 1e-15, 1 - 1e-15)\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "    \n",
    "    def loss_gradient(self, h, y):\n",
    "        h = np.clip(h, 1e-15, 1 - 1e-15)\n",
    "        return -(h/y) + (1-h)/(1-y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        hidden_input = X.dot(self.hiddenWeight) + self.BiasHidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        output_layer_input = hidden_output.dot(self.hiddenWeight) + self.BiasOutput\n",
    "        y_pred = self.softmax(output_layer_input)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.initial_weights(X, y)\n",
    "        n_epoch = 1\n",
    "        \n",
    "        while(n_epoch <= self.epoch):\n",
    "            \n",
    "            # Forward Pass\n",
    "            #hidden Layer\n",
    "            hidden_input = X.dot(self.hiddenWeight) + self.BiasHidden\n",
    "            hidden_output = self.sigmoid(hidden_input)\n",
    "            #output layer\n",
    "            output_layer_input = hidden_output.dot(self.outputWeight) + self.BiasOutput\n",
    "            y_pred = self.softmax(output_layer_input)\n",
    "            \n",
    "            #Backward Pass\n",
    "            #Output Layer Gradient\n",
    "            grad_out_input = self.loss_gradient(y, y_pred) * self.softmax_gradient(output_layer_input)\n",
    "            grad_output = hidden_output.T.dot(grad_out_input)\n",
    "            grad_biasoutput = np.sum(grad_out_input,axis=0,keepdims=True)\n",
    "            #Hidden Layer Gradient\n",
    "            grad_input_out = grad_out_input.dot(self.outputWeight.T) * self.sigmoid_derivative(hidden_input)\n",
    "            grad_input = X.T.dot(grad_input_out)\n",
    "            grad_biasinput = np.sum(grad_input_out, axis=0, keepdims=True)\n",
    "            \n",
    "            #Updating Weights\n",
    "            self.outputWeight -= self.learning_rate * grad_output\n",
    "            self.BiasOutput -= self.learning_rate *grad_biasoutput\n",
    "            self.hiddenWeight -= self.learning_rate * grad_input\n",
    "            self.BiasHidden -= self.learning_rate * grad_biasinput\n",
    "            \n",
    "            \n",
    "            n_epoch += 1\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efebdf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = load_digits()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    \n",
    "    #Normalize X\n",
    "    X1 = X.copy()\n",
    "    X1 = (X1 - X1.mean())/X1.std()\n",
    "    \n",
    "    #Categorize Y\n",
    "    y1 = np.zeros((y.shape[0], (np.amax(y)+1)))\n",
    "    y1[np.arange(y.shape[0]), y] = 1\n",
    "    \n",
    "    #Train-Test Split\n",
    "    split_i = len(y1) - int(len(y1) // (1 / 0.2))\n",
    "    X_train, X_test = X1[:split_i], X1[split_i:]\n",
    "    y_train, y_test = y1[:split_i], y1[split_i:]\n",
    "    \n",
    "    \n",
    "    clf = MultiLayerPerceptron(hidden_layer = 16, epoch=1000, learning_rate=0.01)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = np.argmax(clf.predict(X_test), axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "410000b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (359,16) and (64,16) not aligned: 16 (dim 1) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m clf \u001b[38;5;241m=\u001b[39m MultiLayerPerceptron(hidden_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     22\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> 23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     43\u001b[0m hidden_input \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhiddenWeight) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBiasHidden\n\u001b[0;32m     44\u001b[0m hidden_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(hidden_input)\n\u001b[1;32m---> 45\u001b[0m output_layer_input \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhiddenWeight\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBiasOutput\n\u001b[0;32m     46\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(output_layer_input)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (359,16) and (64,16) not aligned: 16 (dim 1) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
